'''
Author: Generated by GitHub Copilot (GPT-5.1)
Date: 2026-01-23
FilePath: \PC\env\pemfc_chp_mpc.py
Description: 基于 PEMFC-CHP 物理模型的简单 MPC 控制器，用于与强化学习策略对比。
说明：
- 使用与 PEMFCCHPEnv 相同的物理参数和负荷数据
- 将 15 个堆栈电流密度假定为相同的标量 j（简化优化维度）
- 采用随机采样的有限时域 MPC（无外部优化库依赖）
'''

import os
import copy
import numpy as np
import math
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

from pemfc_chp_train import PEMFCCHPEnv

# 解决中文显示问题
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


class MPCState:
    """MPC 内部使用的系统状态（与 RL 环境解耦）。"""

    def __init__(self, step: int, T_tank: float, SOC: float,
                 fc_V_loss: float, fc_SOH: float, P_fc_prev: float):
        self.step = step
        self.T_tank = T_tank
        self.SOC = SOC
        self.fc_V_loss = fc_V_loss
        self.fc_SOH = fc_SOH
        self.P_fc_prev = P_fc_prev

    def copy(self):
        return copy.deepcopy(self)


class PolicyNet(nn.Module):
    """小型前馈网络：输入状态特征，输出标量 j 的预测均值（在训练中用 MSE 拟合最佳首动作）。"""
    def __init__(self, in_dim: int, hidden: int = 64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, 1),
        )

    def forward(self, x):
        return self.net(x).squeeze(-1)


def state_to_feature(p, load_profiles, state: MPCState):
    """将 `MPCState` 与当前负荷编码为网络特征向量。与 env 观测类似，但维度更小。

    特征包含：`P_need(kW), Q_need(kW), T_tank(K), SOC, P_fc_prev(kW)`
    返回 numpy float32 向量
    """
    idx = min(state.step, len(load_profiles) - 1)
    P_need = float(load_profiles['P_e'].iloc[idx])  # kW
    Q_need = float(load_profiles['Q_th'].iloc[idx])  # kW
    feat = np.array([
        P_need,
        Q_need,
        state.T_tank,
        state.SOC,
        state.P_fc_prev / 1000.0,
    ], dtype=np.float32)
    return feat


def _get_load(p, load_profiles, step_idx):
    idx = min(step_idx, len(load_profiles) - 1)
    P_need = load_profiles['P_e'].iloc[idx] * 1000.0
    Q_need = load_profiles['Q_th'].iloc[idx] * 1000.0
    return P_need, Q_need


def _run_system_physics(p, state: MPCState, j_scalar: float):
    """与环境中的 _run_system_physics 等价，但假定所有堆栈电流密度相同。"""
    N_stacks = p['N_stacks']

    # 考虑衰减：电压基准随累积电压降减小
    V_base = p['V_nom'] - (state.fc_V_loss / p['N_cell'])

    P_tot, Q_tot, H2_tot = 0.0, 0.0, 0.0

    if j_scalar <= 1e-4:
        return P_tot, Q_tot, H2_tot

    j = float(np.clip(j_scalar, p['J_min'], p['J_max']))
    I = j * p['A_fc']
    V_cell = max(0.45, V_base - j * p['R_internal'])

    P_stack = p['N_cell'] * V_cell * I
    Q_stack = p['N_cell'] * I * (1.48 - V_cell)
    mol_H2 = p['N_cell'] * I / (2 * p['F'])

    P_tot = N_stacks * P_stack
    Q_tot = N_stacks * Q_stack
    H2_tot = N_stacks * mol_H2

    return P_tot, Q_tot, H2_tot


def _update_SOC(p, SOC_old, P_batt_charge_from_fc, P_batt_discharge_output):
    """基于能量守恒的 SOC 更新（与环境实现保持一致）。

    参数：
    - p: 参数字典
    - SOC_old: 旧 SOC
    - P_batt_charge_from_fc: 燃料电池给电池的充电功率（W，正）
    - P_batt_discharge_output: 电池对外输出的放电功率（W，正）
    """
    capacity_Wh = p['C_batt_Ah'] * p['V_batt']
    dt_hr = p['dt'] / 3600.0

    eta_total = p.get('eta_batt', 1.0)
    eta_ch = math.sqrt(max(eta_total, 0.0))
    eta_dis = eta_ch

    E_old_Wh = SOC_old * capacity_Wh
    E_ch_Wh = eta_ch * max(P_batt_charge_from_fc, 0.0) * dt_hr
    E_dis_internal_Wh = (max(P_batt_discharge_output, 0.0) / max(eta_dis, 1e-8)) * dt_hr
    E_new_Wh = E_old_Wh + E_ch_Wh - E_dis_internal_Wh

    SOC_new = E_new_Wh / max(capacity_Wh, 1e-9)
    return float(np.clip(SOC_new, 0.0, 1.0))


def _thermal_management(p, state: MPCState, Q_gen, Q_need, P_need, P_gen_raw, alpha=0.5):
    """复制环境中的 _thermal_management 逻辑（alpha 目前未实际使用）。"""
    # 1. 当前水箱可用热量 & 热损失
    Q_tank_avail = (
        p['M_water'] * p['cp_water_liq'] *
        (state.T_tank - p['T_tank_target'] - p['T_band'])
    ) / p['dt']

    Q_loss = (state.T_tank - p['T_amb']) / p['R_air_thermal']

    Q_deficit = Q_need - Q_tank_avail + Q_loss
    Q_gen_used = min(Q_deficit, Q_gen)
    Q_deficit -= Q_gen_used

    # 2. PEMFC 电功率富余
    P_surplus_fc = max(P_gen_raw - P_need, 0.0)

    # 3. SOC → 功率换算
    # 将 1.0 SOC 对应到在一个时间步长内的可转换功率 (W)
    soc_to_w = (
        p['C_batt_Ah'] * p['V_batt'] /
        (p['dt'] / 3600.0)
    )

    SOC_target = p['SOC_target']
    SOC_band = p['SOC_band']
    SOC_lower = SOC_target - SOC_band
    SOC_upper = SOC_target + SOC_band  # 保留以便后续扩展
    SOC_emergency = max(SOC_lower - 0.05, 0.0)

    # 4. SOC 主动恢复（只计算充电功率）
    P_charge = 0.0
    if P_surplus_fc > 0.0:
        P_charge = min(
            (SOC_target - state.SOC) * soc_to_w,
            P_surplus_fc
        )
        P_surplus_fc -= P_charge

    # 5. 电池最大可放电功率
    if Q_deficit > 0 and state.T_tank < p['T_tank_target']:
        P_batt_max = max(
            (state.SOC - SOC_emergency) * soc_to_w,
            0.0
        )
    else:
        P_batt_max = max(
            (state.SOC - SOC_lower) * soc_to_w,
            0.0
        )

    # 6. 电加热棒功率
    if Q_deficit > 0:
        P_heater_need = Q_deficit / p['eff_heating_rod']
        P_heater_max = P_surplus_fc + P_batt_max
        P_heater = min(P_heater_need, P_heater_max)
        P_heater_fc = min(P_heater, P_surplus_fc)
        P_heater_batt = P_heater - P_heater_fc
    else:
        P_heater_fc = 0.0
        P_heater_batt = 0.0
        P_heater = 0.0

    Q_heater = P_heater * p['eff_heating_rod']

    return P_heater_fc, P_heater_batt, Q_heater, P_charge, Q_loss, Q_gen_used


def _update_PEMFC_SOH(p, state: MPCState, P_fc_now):
    """复制环境中的 _update_PEMFC_SOH 逻辑。"""
    dt_hr = p['dt'] / 3600.0

    delta_V = 0.0

    # 启停损耗
    if state.P_fc_prev <= 10.0 and P_fc_now > 10.0:
        delta_V += p['kappa_1']

    # 变载损耗
    delta_P_kw = abs(P_fc_now - state.P_fc_prev) / 1000.0
    delta_V += p['kappa_2'] * delta_P_kw

    # 稳态运行损耗
    if 0 < P_fc_now < p['P_fc_low']:
        delta_V += p['kappa_3'] * dt_hr
    elif P_fc_now > p['P_fc_high']:
        delta_V += p['kappa_4'] * dt_hr

    V_EOL = 0.11
    delta_SOH_fc = delta_V / V_EOL

    fc_V_loss_new = state.fc_V_loss + delta_V
    fc_SOH_new = state.fc_SOH - delta_SOH_fc

    return delta_SOH_fc, fc_V_loss_new, fc_SOH_new


def _compute_reward(p, P_gen, P_need, next_SOC, next_T, P_heater, delta_SOH):
    """复制环境中的 _compute_reward 逻辑。"""
    reward = 0.0

    T_err = abs(next_T - p['T_tank_target'])
    P_err = abs(P_gen - P_need)
    SOC_err = abs(next_SOC - p['SOC_target'])

    reward += P_heater / 5000.0 * p['wP']

    reward += 200.0 * np.exp(-0.002 * P_err)
    reward += 400.0 * np.exp(-10.0 * SOC_err)
    reward += 500.0 * np.exp(-0.1 * T_err)

    T_high = p['T_tank_target'] + p['T_band']
    T_low = p['T_tank_target'] - p['T_band']

    if next_T < T_low:
        reward -= 50.0 * (T_low - next_T)
    elif next_T > T_high:
        reward -= 50.0 * (next_T - T_high)
    else:
        reward += 50.0

    if abs(next_SOC - p['SOC_target']) > 0.1:
        reward -= 30.0 * 25.0 * abs(next_SOC - p['SOC_target'])

    reward -= delta_SOH * 1e7

    return float(reward)


def simulate_one_step(p, load_profiles, state: MPCState, j_scalar: float):
    """在给定状态和标量电流密度 j 下，模拟一步物理过程，返回下一状态、奖励和 info。"""
    # 负荷
    P_need, Q_need = _get_load(p, load_profiles, state.step)

    # PEMFC 原始输出
    P_gen_raw, Q_gen, mol_H2 = _run_system_physics(p, state, j_scalar)

    # 热管理
    P_heater_fc, P_heater_batt, Q_heater, P_charge, Q_loss, Q_gen_used = _thermal_management(
        p=p,
        state=state,
        Q_gen=Q_gen,
        Q_need=Q_need,
        P_need=P_need,
        P_gen_raw=P_gen_raw,
        alpha=0.5,
    )

    # 电功率收支（系统对外净输出，不含电池补电）
    P_gen = P_gen_raw - P_heater_fc - P_charge

    # SOC 给电需求做补充
    SOC_to_fc_output = 0.0
    if P_gen < P_need:
        P_deficit = P_need - P_gen
        SOC_target = p['SOC_target']
        SOC_band = p['SOC_band']
        SOC_lower = SOC_target - SOC_band
        SOC_emergency = max(SOC_lower - 0.05, 0.0)
        SOC_min = SOC_emergency

        capacity_Wh = p['C_batt_Ah'] * p['V_batt']
        eta_total = p.get('eta_batt', 1.0)
        eta_dis = math.sqrt(max(eta_total, 0.0))

        available_energy_Wh = max(state.SOC - SOC_min, 0.0) * capacity_Wh
        max_output_power_W = available_energy_Wh * eta_dis * 3600.0 / p['dt']

        SOC_to_fc_output = min(max_output_power_W, P_deficit)
        P_gen += SOC_to_fc_output

    # 更新水箱温度
    Q_net = Q_gen_used + Q_heater - Q_need - Q_loss
    next_T = state.T_tank + (
        Q_net * p['dt'] /
        (p['M_water'] * p['cp_water_liq'])
    )
    next_T = float(np.clip(next_T, 273.15, 373.15))

    # 更新 SOC
    next_SOC = _update_SOC(
        p=p,
        SOC_old=state.SOC,
        P_batt_charge_from_fc=P_charge,
        P_batt_discharge_output=(P_heater_batt + SOC_to_fc_output),
    )

    # 更新 SOH
    delta_SOH, fc_V_loss_new, fc_SOH_new = _update_PEMFC_SOH(p, state, P_gen_raw)

    # 奖励
    reward = _compute_reward(
        p=p,
        P_gen=P_gen,
        P_need=P_need,
        next_SOC=next_SOC,
        next_T=next_T,
        P_heater=P_heater_fc + P_heater_batt,
        delta_SOH=delta_SOH,
    )

    # 构造下一状态
    next_state = MPCState(
        step=state.step + 1,
        T_tank=next_T,
        SOC=next_SOC,
        fc_V_loss=fc_V_loss_new,
        fc_SOH=fc_SOH_new,
        P_fc_prev=P_gen_raw,
    )

    info = {
        'SOC': next_SOC,
        'T_tank_C': next_T - 273.15,
        'SOH': fc_SOH_new,
        'P_gen_W': P_gen,
        'P_gen_KW': P_gen / 1000.0,
        'P_gen_KWh': (P_gen / 1000.0) * (p['dt'] / 3600.0),
        'P_need_W': P_need,
        'P_need_KW': P_need / 1000.0,
        'P_need_KWh': (P_need / 1000.0) * (p['dt'] / 3600.0),
        'P_raw_W': P_gen_raw,
        'P_raw_KW': P_gen_raw / 1000.0,
        'P_raw_KWh': (P_gen_raw / 1000.0) * (p['dt'] / 3600.0),
        'H2_rate_mol_s': mol_H2,
        'Q_gen_W': Q_gen,
        'Q_gen_KW': Q_gen / 1000.0,
        'Q_gen_KWh': (Q_gen / 1000.0) * (p['dt'] / 3600.0),
        'Q_need_W': Q_need,
        'Q_need_KW': Q_need / 1000.0,
        'Q_need_KWh': (Q_need / 1000.0) * (p['dt'] / 3600.0),
        'Q_heater_W': Q_heater,
        'Q_heater_KW': Q_heater / 1000.0,
        'Q_heater_KWh': (Q_heater / 1000.0) * (p['dt'] / 3600.0),
        'Q_loss_W': Q_loss,
        'Q_loss_KW': Q_loss / 1000.0,
        'Q_loss_KWh': (Q_loss / 1000.0) * (p['dt'] / 3600.0),
        'Q_gen_used_W': Q_gen_used,
        'Q_gen_used_KW': Q_gen_used / 1000.0,
        'Q_gen_used_KWh': (Q_gen_used / 1000.0) * (p['dt'] / 3600.0),
        'reward': reward,
    }

    return next_state, reward, info


def mpc_control_step(p, load_profiles, state: MPCState,
                     horizon: int = 5,
                     n_candidates: int = 200,
                     rng: np.random.Generator | None = None,
                     policy_net: nn.Module | None = None,
                     policy_sigma: float = 0.05,
                     replay: list | None = None,
                     train_policy: bool = False,
                     optimizer: optim.Optimizer | None = None,
                     policy_batch: int = 64,
                     policy_train_steps: int = 1,
                     policy_train_freq: int = 10,
                     step_counter: int = 0):
    """
    简单随机采样 MPC 控制器（Receding‑horizon, sampling‑based MPC）。

    算法概述：
    1. 在当前时刻基于 `state` 随机采样 `n_candidates` 条未来控制序列（每条序列长度为 `horizon`），
       这里每个控制输入是一个标量电流密度 `j`（函数假设所有堆栈采用相同 j）。
    2. 对每条候选序列使用物理模型（`simulate_one_step`）从当前状态向前滚动预测 `horizon` 步，累计每步奖励。
    3. 选择累计奖励最高的候选序列，返回该序列的首个控制量 `j_0`（这是标准的 receding‑horizon 策略）。

    参数：
    - p: 参数字典（与 `PEMFCCHPEnv.p` 相同），用于物理仿真与约束边界。
    - load_profiles: 负荷数据帧（与环境中 load_profiles 相同）。
    - state: 当前 `MPCState` 对象（注意：函数内部会对其做拷贝进行预测，原 `state` 不会被修改）。
    - horizon: 预测时域长度（整数，默认 5）。
    - n_candidates: 每步采样的候选序列数（越大越稳但越慢，默认 200）。
    - rng: 可选的 `np.random.Generator` 实例，用于可复现采样；若为 None 会创建新的默认生成器。

    返回值：
    - best_u0: 选中序列的首个控制量（float，电流密度 j）。
    - best_return: 对应的累计预测奖励（float）。

    复杂度：O(n_candidates * horizon * cost_simulate_step)。

    注意/建议：
    - `state.copy()` 必须返回与原 `state` 相互独立的副本（当前实现使用 `deepcopy`）。
    - 若需要更高维控制（例如对每个堆做不同 j），则采样维度将指数增长，此时应切换到更高效的优化器（CEM、均值迁移、梯度法等）。
    - 为了更稳定的比较，建议在外部传入一个固定的 `rng`（用于可复现对比实验）。

    实现细节与注释：
    """
    if rng is None:
        rng = np.random.default_rng()

    # 控制量取值范围
    J_min = p['J_min']
    J_max = p['J_max']

    best_return = -np.inf
    best_u0 = 0.0

    # 对每个候选序列进行采样与前向模拟评估
    # 预先将基于 state 的特征计算一次（用于 policy_net 或后续训练样本）
    feat = state_to_feature(p, load_profiles, state)

    for _ in range(n_candidates):
        # 采样策略：若传入 policy_net，则以网络输出为均值做高斯采样（更集中的候选），
        # 否则使用均匀采样。每个候选是长度为 horizon 的序列。
        if policy_net is not None:
            with torch.no_grad():
                mu = float(policy_net(torch.from_numpy(feat).unsqueeze(0)).cpu().numpy()[0])
            # mu 以 single-stack j 为单位（与网络训练目标一致）
            u_seq = rng.normal(loc=mu, scale=policy_sigma, size=horizon)
        else:
            u_seq = rng.uniform(J_min, J_max, size=horizon)

        # 复制状态用于预测；必须与原 state 隔离
        tmp_state = state.copy()
        R = 0.0

        # 对该候选序列做 horizon 步前向模拟，累计奖励
        for h in range(horizon):
            # 如果已经到负荷序列末尾则提前终止预测
            if tmp_state.step >= len(load_profiles):
                break
            # simulate_one_step 返回 (next_state, reward, info)
            tmp_state, r, _ = simulate_one_step(p, load_profiles, tmp_state, u_seq[h])
            R += r

        # 记录最佳候选
        if R > best_return:
            best_return = R
            best_u0 = float(u_seq[0])

    # 若启用了在线训练（replay 非空 且 train_policy True），将样本加入 replay 并按设定频率训练
    if replay is not None:
        replay.append((feat, best_u0, best_return))

    if train_policy and policy_net is not None and optimizer is not None and replay is not None:
        # 每 policy_train_freq 步进行训练（step_counter 从外部 run loop 传入）
        if (step_counter % policy_train_freq) == 0 and len(replay) >= min(policy_batch, 8):
            # 随机采样小批量
            batch_idx = np.random.choice(len(replay), size=min(policy_batch, len(replay)), replace=False)
            feats = np.stack([replay[i][0] for i in batch_idx]).astype(np.float32)
            targets = np.array([replay[i][1] for i in batch_idx], dtype=np.float32)

            policy_net.train()
            for _ in range(policy_train_steps):
                optimizer.zero_grad()
                inputs = torch.from_numpy(feats)
                preds = policy_net(inputs)
                loss = nn.functional.mse_loss(preds, torch.from_numpy(targets))
                loss.backward()
                optimizer.step()

    return best_u0, best_return


def run_mpc_episode(horizon: int = 5, n_candidates: int = 200, seed: int = 0,
                    use_policy: bool = False,
                    policy_sigma: float = 0.05,
                    policy_lr: float = 1e-3,
                    policy_batch: int = 64,
                    policy_train_freq: int = 10,
                    policy_train_steps: int = 1):
    """运行一个完整的 MPC 回合，返回 history 列表（每步一个 info 字典）。"""
    base_env = PEMFCCHPEnv()
    p = base_env.p
    load_profiles = base_env.load_profiles

    rng = np.random.default_rng(seed)

    # 初始状态与 RL 环境保持一致
    state = MPCState(
        step=0,
        T_tank=60.0 + 273.15,
        SOC=p['SOC_init'],
        fc_V_loss=0.0,
        fc_SOH=1.0,
        P_fc_prev=0.0,
    )

    n_steps = len(load_profiles)
    history = []

    # 如果启用了策略网络，则创建并准备在线训练工具
    policy_net = None
    optimizer = None
    replay = []
    if use_policy:
        # 输入维度与 state_to_feature 保持一致（P_need, Q_need, T_tank, SOC, P_fc_prev_kW）
        policy_net = PolicyNet(in_dim=5, hidden=64)
        optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)

    print(f">>> MPC 仿真开始，步数={n_steps}，预测时域={horizon}，候选序列数={n_candidates}, use_policy={use_policy}")

    step_counter = 0
    while state.step < n_steps:
        j_opt, J_val = mpc_control_step(
            p=p,
            load_profiles=load_profiles,
            state=state,
            horizon=horizon,
            n_candidates=n_candidates,
            rng=rng,
            policy_net=policy_net,
            policy_sigma=policy_sigma,
            replay=replay,
            train_policy=use_policy,
            optimizer=optimizer,
            policy_batch=policy_batch,
            policy_train_steps=policy_train_steps,
            policy_train_freq=policy_train_freq,
            step_counter=step_counter,
        )

        next_state, reward, info = simulate_one_step(p, load_profiles, state, j_opt)
        history.append(info)

        state = next_state
        step_counter += 1

    print(">>> MPC 仿真结束")
    return history


def _summary_and_plots(history, prefix: str = "MPC"):
    """与 pemfc_chp_test.plot_analysis 类似的统计与绘图，用于评估 MPC 性能。"""
    history = np.array(history, dtype=object)

    P_gen_final = np.array([x['P_gen_KW'] for x in history])
    P_load = np.array([x['P_need_KW'] for x in history])
    P_raw_KW = np.array([x['P_raw_KW'] for x in history])

    H2_rate_mol_s = np.array([x['H2_rate_mol_s'] for x in history])

    Q_gen = np.array([x['Q_gen_KW'] for x in history])
    Q_heater = np.array([x['Q_heater_KW'] for x in history])
    Q_load = np.array([x['Q_need_KW'] for x in history])
    Q_loss = np.array([x['Q_loss_KW'] for x in history])
    Q_gen_used = np.array([x['Q_gen_used_KW'] for x in history])

    SOC = np.array([x['SOC'] for x in history])
    SOH = np.array([x['SOH'] for x in history])
    T_tank_C = np.array([x['T_tank_C'] for x in history])

    reward_list = np.array([x['reward'] for x in history])

    t = np.arange(len(P_gen_final))

    # 电跟踪率
    eta_e_t = P_gen_final / (P_load + 1e-6)
    eta_e_avg = np.mean(eta_e_t)

    # 热功率指标
    Q_supply = Q_gen_used + Q_heater
    eta_h_t = Q_supply / (Q_load + 1e-6)
    eta_h_avg = np.mean(eta_h_t)

    # SOC 指标
    SOC_mean = np.mean(SOC)
    SOC_min = np.min(SOC)
    SOC_max = np.max(SOC)

    # SOH 指标
    SOH_init = SOH[0]
    SOH_final = SOH[-1]
    SOH_decay = SOH_init - SOH_final
    SOH_decay_rate = SOH_decay / len(SOH)

    # 水箱温度指标
    T_mean = np.mean(T_tank_C)
    T_min = np.min(T_tank_C)
    T_max = np.max(T_tank_C)

    # 整体效率（基于氢气 HHV）
    HHV_H2 = 285850.0  # J/mol，与训练环境保持一致
    E_h = H2_rate_mol_s * HHV_H2 / 1000.0  # kW

    E_e_useful = np.sum(np.minimum(P_gen_final, P_load))
    E_h_useful = np.sum(np.minimum(Q_supply, Q_load))

    eta_overall = (E_e_useful + E_h_useful) / (np.sum(E_h) + 1e-6)

    print("========== MPC Performance Summary ==========")
    print(f"Average Electric Tracking Rate η_e: {eta_e_avg:.3f}")
    print(f"Average Thermal  Tracking Rate η_h: {eta_h_avg:.3f}")
    print(f"Overall Energy Efficiency η: {eta_overall:.3f}")
    print(f"SOC: mean={SOC_mean:.3f}, min={SOC_min:.3f}, max={SOC_max:.3f}")
    print(f"SOH decay: {SOH_decay:.5f}, decay rate per step: {SOH_decay_rate:.5e}")
    print(f"T_tank: mean={T_mean:.2f} °C, min={T_min:.2f}, max={T_max:.2f}")

    # 绘图（文件名加前缀避免与 RL 结果混淆）
    plt.figure(figsize=(10, 4))
    plt.plot(t, P_load, label="Electric Load", linestyle="--")
    plt.plot(t, P_gen_final, label=f"{prefix} Net Power")
    plt.xlabel("Time step")
    plt.ylabel("Power (kW)")
    plt.title(f"{prefix} Electrical Power")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{prefix}_电功率曲线.png", dpi=300)

    plt.figure(figsize=(10, 4))
    plt.plot(t, Q_load + Q_loss, label="Heat Demand + Loss", linestyle="--")
    plt.plot(t, Q_heater + Q_gen, label=f"{prefix} Heater + PEMFC")
    plt.xlabel("Time step")
    plt.ylabel("Power (kW)")
    plt.title(f"{prefix} Thermal Power")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{prefix}_热功率曲线.png", dpi=300)

    plt.figure(figsize=(10, 4))
    plt.plot(t, SOC)
    plt.xlabel("Time step")
    plt.ylabel("SOC")
    plt.title(f"{prefix} Battery SOC")
    plt.grid(True)
    plt.savefig(f"{prefix}_电池SOC曲线.png", dpi=300)

    plt.figure(figsize=(10, 4))
    plt.plot(t, SOH)
    plt.xlabel("Time step")
    plt.ylabel("SOH")
    plt.title(f"{prefix} PEMFC Stack SOH")
    plt.grid(True)
    plt.savefig(f"{prefix}_PEMFC堆寿命SOH曲线.png", dpi=300)

    plt.figure(figsize=(10, 4))
    plt.plot(t, T_tank_C)
    plt.xlabel("Time step")
    plt.ylabel("Temperature (°C)")
    plt.title(f"{prefix} Water Tank Temperature")
    plt.grid(True)
    plt.savefig(f"{prefix}_水箱温度曲线.png", dpi=300)

    plt.figure(figsize=(10, 4))
    plt.plot(t, reward_list)
    plt.xlabel("Time step")
    plt.ylabel("Reward")
    plt.title(f"{prefix} Reward per Step")
    plt.grid(True)
    plt.savefig(f"{prefix}_每步奖励reward曲线.png", dpi=300)

    plt.close('all')


def main():
    os.system('cls' if os.name == 'nt' else 'clear')

    # 可以根据需要调整 MPC 参数
    horizon = 5
    n_candidates = 200
    seed = 0

    history = run_mpc_episode(horizon=horizon, n_candidates=n_candidates, seed=seed)

    # 保存结果，便于与 RL 的 test_results.npy 做对比
    np.save("mpc_results.npy", np.array(history, dtype=object))

    _summary_and_plots(history, prefix=f"MPC_H{horizon}")

    print(">>> MPC 结果已保存到 mpc_results.npy，并生成若干 PNG 曲线图。")


if __name__ == "__main__":
    main()
